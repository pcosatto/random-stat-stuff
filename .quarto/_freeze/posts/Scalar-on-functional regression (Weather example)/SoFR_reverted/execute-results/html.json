{
  "hash": "2dcacd81ea296b568097a5d2f4e6f67d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Scalar-on-functional Regression\"\ndescription: \"Weather example\"\nauthor: \"Pedro Cosatto\"\ndate: \"May 2025\"\ncategories:\n  - functional data analysis\n  - regression\nformat:\n  html:\n    html-math-method: mathjax\n    include-before-header:\n      - file: mathjax-config.html\n---\n\n\n\n### Introduction\n\nWe work with weather data from 76 weather stations in Argentina, from a [public data source](https://www.datos.gob.ar/), in which several variables were measured during a year. In the next plot we can see the mean monthly temperatures:\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](SoFR_reverted_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=384}\n:::\n:::\n\n\nLet $\\mathbf{y}(t)$ be a vector that contains temperatures at time $t \\in \\mathcal{T}$ for the $N = 76$ locations. Each of the temperature functions are represented by uppercase $Y_i(t)$.\n\nImagine our goal is to predict the temperature curve of a new station, only with some geographical information such as lattitude, longitude, altitude above sea level, and region. In this case **we would need to use a regression model in which the explained variable is actually *a whole function***, in contrast to a classic regression in which we would be interested in point-wise predictions for a given month. We could also use such a model to study with some detail **how the different explanatory variables are related to the temperature curves**, or some higher level details such as how those covariates explain the *change* in temperature, by looking at its derivative. This models, in which the response is a random function and covariates are still treated as numeric variables, are known as **Scalar-on-functional (SoF)** regressions.\n\nThis work is based on Chapters 5 and 13 of the text *Functional Data Analysis* by Ramsay-Silverman.\n\n### First step - Smoothing response observations\n\nOur target curves are only observed in finitely many ($n=12$) points. The lines in the previous plot were drawn by simple linear interpolation, but we need to build higher quality realizations, with some degree of smoothness as data input to our regression model. Typically, continuity in at least the second derivative is required.\n\nLet $\\mathbf{Y} \\in \\mathbb{R}^{N \\times n}$ be the temperature data matrix, we create curves choosing a basis expansion, and the number $K_Y$ of coefficients. Typical basis expansions are Fourier, cubic B-splines and wavelets. This expansions are useful because we create complex non-linear function using a linear structure. Let $\\boldsymbol{\\Phi} \\in \\mathbb{R}^{n \\times K_{Y}}$ be the matrix that contains the basis functions evaluated in the points in $\\mathcal{T}$ that were observed in the random sample. This is treated as a 'design matrix' in the smoothing process. Some built-in functions of the package `fda` are used:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nK_y <- 7 #Value used in text\nbasis_Y <- create.fourier.basis(rangeval = c(0,12), nbasis = K_y)\nPhi <- eval.basis(1:12, basis_Y)\n```\n:::\n\n\nFourier basis were used, since our mean temperature functions are periodic. Let $\\mathbf{C} \\in \\mathbb{R}^{N \\times K_Y}$ be the linear coefficients that link each term of the basis expansion to the individuals in the sample. The underlying (multivariate) linear model is\n\n$$\n\\mathbf{y}(t) = \\mathbf{C} \\phi(t) + \\epsilon(t)\n$$\n\nwhere $\\epsilon$ is a random measurement error vector term, and the vector $\\phi_i(t) \\in \\mathbb{R}^{K_Y}$ is the i-th row of $\\mathbf{\\Phi}$. Classical methods for estimating $\\mathbf{C}$ include regularized least squares or kernel smoothing. For simplicity, we use classic linear regression here:\n\n$$\n\\hat{\\mathbf{C}} = \\mathbf{Y} \\boldsymbol{\\Phi} (\\boldsymbol{\\Phi}' \\boldsymbol{\\Phi})^{-1}\n$$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndim(Y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 76 12\n```\n\n\n:::\n\n```{.r .cell-code}\nC <- as.matrix(Y) %*% Phi %*% solve(t(Phi) %*% Phi)\n```\n:::\n\n\nAs mentioned before, this matrix will be an input for the main regression problem, since it contains all the information about the temperature curves. For some $t_0 \\in \\mathcal{T}$, we can evaluate all the functions $Y_i(t_0)$ by applying the basis coefficients at a desired point $t_0$\n\n$$\n\\hat{\\mathbf{y}}(t_0) = \\hat{\\mathbf{C}} \\phi(t_0)\n$$\n\n#### Is previous smoothing necessary?\n\nIf we are intended to predict or estimate temperature only in the 12 months, it may not be necessary to create smooth continuous curves of temperature. However, if we are interested in extracting more information about those functions, such as derivatives (for example, analyzing the rate of change in temperature across stations given the covariates), we need well defined, smooth curves.\n\n### Second step - Main course: The regression itself\n\nLet's look ad the ingredients we need to construct our main SoF regression.\n\n#### Input data for Y\n\nFirst, the input data for the response variable (remember, our response variables are functions!). We could use $\\mathbf{Y}$ here, plain and simple, but we have already discussed that we *need* previous smoothing, so our actual input will be $\\mathbf{\\hat{C}} \\phi(t)$, for as many $t$ values as we want, we don't have to specify (spoiler: we will integrate over $\\mathcal{T}$). Here it is clear that our only real input is the matrix $\\mathbf{\\hat{C}}$.\n\n#### Input data for our covariates (a.k.a. the *Xs*)\n\nInput data for our explanatory variables will be organized in a design matrix $\\mathbf{X} \\in \\mathbb{R}^{N \\times p}$. Here we choose `Region` (categorical), `Latitude`, `Longitude` and `Altitude` (all numeric). We could add interactions as well. Here, nothing differs from other classical experimental design problems. If we chose the categorical variable only, what we would be doing is nothing more than a functional version of one-way ANOVA.\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Covariate design matrix\nX <- model.matrix(~ Region + Latitude + Longitude + Altitude)\n```\n:::\n\n\nWe can have a look of what the $\\mathbf{X}$ matrix looks like:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n   (Intercept) Region1 Region2 Region3 Region4 Region5 Region6 Latitude\n1            1       0       0       0       0       1       0   -22.10\n2            1       0       0       0       0       1       0   -23.15\n3            1       0       0       0       0       1       0   -22.52\n4            1       0       0       0       0       1       0   -24.24\n5            1       0       0       0       0       1       0   -24.85\n6            1       0       0       0       1       0       0   -24.71\n7            1       0       0       0       1       0       0   -25.74\n8            1       0       0       0       0       1       0   -26.84\n9            1       0       0       0       0       1       0   -27.77\n10           1       0       0       0       1       0       0   -27.45\n   Longitude Altitude\n1     -65.60     3462\n2     -64.33      359\n3     -63.80      450\n4     -65.27     1239\n5     -65.48     1238\n6     -60.58      130\n7     -54.47      270\n8     -65.10      450\n9     -64.31      199\n10    -59.06       52\n```\n\n\n:::\n:::\n\n\n#### The linear model\n\nPutting everything into place we have\n\n$$\n\\mathbf{y}(t) = \\mathbf{X} \\boldsymbol{\\beta}(t) + \\boldsymbol{\\varepsilon}(t)\n$$\n\nwhere $\\beta$ is now a vector function of the coefficients, that is the main parameter that we need to estimate here. We can also use basis expansions, to create complex non-linear functions. We use the following notation\n\n$$\n\\beta(t) = \\mathbf{B} \\theta(t)\n$$\n\nwhere $\\theta$ has $K_{\\beta}$ components, also determined by the user. In this case we choose cubic B-splines:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Basis for beta functions (B-splines)\nK_beta <- 8\nbasis_beta <- create.bspline.basis(rangeval = c(0,12), nbasis = K_beta)\n```\n:::\n\n\nThe error term $\\varepsilon(t)$ is a vector function with zero mean and covariance $\\Sigma_{\\varepsilon}$. We assume there is no covariance structure between observations, and also that the covariance is constant in $\\mathcal{T}$. This is a strong assumption but we want to keep things simple. There are some possible structures that we can impose on this matrix to model certain things. Summing up, our final model is\n\n$$\n\\hat{\\mathbf{C}} \\phi(t) = \\mathbf{X} \\mathbf{B} \\theta(t) + \\boldsymbol{\\varepsilon}(t)\n$$\n\n#### A roughness penalization\n\nLet's introduce a roughness penalization on $\\beta(t)$, that is, we want our coefficient function to have some degree of smoothness, and we would like to introduce this explicitly as a new parameter of the model. This can be stated as\n\n$$\n\\int_{\\mathcal{T}} (L \\beta)' (L \\beta)\n$$\n\nwhere the matrix $L$ represents some linear differential operator, that multiplied by $\\beta$ gives an approximation of the second derivative. Since we want the functions $\\beta$ not to be very rough, we would like this integral to be relatively small. Notice that, for the first time, we are integrating over $\\mathcal{T}$, which is the analogous of summing over the discrete sample space in classical regression.\n\n#### The loss function\n\nOur optimization problem is\n\n$$\n\\begin{equation}\n\\min_{\\mathbf{B}} \\left\\{\n  \\int_\\mathcal{T} (\\hat{\\mathbf{C}} \\phi - \\mathbf{X} \\mathbf{B} \\theta)' (\\hat{\\mathbf{C}} \\phi - \\mathbf{X} \\mathbf{B} \\theta)\n  + \\lambda \\int_\\mathcal{T} (L \\mathbf{B} \\theta)' (L \\mathbf{B} \\theta)\n\\right\\}\n\\end{equation}\n$$ {#eq-min}\n\nOr, in plain words, to find the linear coefficients $\\hat{\\mathbf{B}}$, that we need to construct an estimate $\\hat{\\beta}(t)$, by minimizing a loss function that introduces a roughness penalty. Here is a summary of pre-specified things that will affect our results:\n\n-   The previous smoothing of the temperature curves, summarized in $\\mathbf{\\hat{C}}$.\n\n-   The basis functions $\\phi$ and $\\theta$. They are different here but we would also want to choose them to be equal.\n\n-   The linear differentiation operator $L$.\n\n-   The number of elements in both basis: $K_Y$ and $K_{\\beta}$.\n\n-   The value of the roughness penalty parameter $\\lambda$.\n\nThe last two can be determined using cross validation or similar techniques. For our convenience we also define some matrices that we will use in the next section:\n\n$$\n\\mathbf{J}_{\\phi\\phi} = \\int \\phi \\phi', \\quad\n\\mathbf{J}_{\\theta\\theta} = \\int \\theta \\theta', \\quad\n\\mathbf{J}_{\\phi\\theta} = \\int \\phi \\theta', \\quad\n\\mathbf{R} = \\int (L\\theta)(L\\theta)'\n$$\n\nThese are all Gran matrices between the different basis terms, integrated over the support. They play a role in the minimization process and in calculating the solution. They can easily be obtained in `fda`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nJ_YY <- inprod(basis_Y, basis_beta)\nJ_BB <- inprod(basis_beta, basis_beta)\nJ_YB <- inprod(basis_Y, basis_beta)\nR <- eval.penalty(basis_beta, int2Lfd(2))\n```\n:::\n\n\n#### The *slope* estimates\n\nBy calcualting the derivative of @eq-min and setting it to 0, and operating with the traces of the Gram matrices above, we get\n\n$$\n\\begin{equation}\n\\text{vec}(\\mathbf{B}) =\n\\left[ \\mathbf{J}_{\\theta\\theta} \\otimes (\\mathbf{X}' \\mathbf{X}) + \\mathbf{R} \\otimes \\lambda I \\right]^{-1}\n\\text{vec}(\\mathbf{X}' \\mathbf{C} \\mathbf{J}_{\\phi\\theta})\n\\end{equation}\n$$ {#eq-b}\n\nHere the notation $\\text{vec}$ expresses a vector that has all the elements of $\\textbf{B}$ stacked by rows, and $\\otimes$ is the Kronecker product. Essentially, for some random matrices A and B, the Kronecker product $A \\otimes B$ is a matrix of matrices that result of multiplying each scalar element $a_{ij}$ with B. Looks coomplicated but it is the best way to summarize the result in linear algebra notation. As we mentioned above, we could have chosen the basis $\\phi$ and $\\theta$ to be equal. Also, we could have chosen different penalization parameters $\\lambda_i$ for every component of $\\beta$. That changes the result slightly, but it's not difficult. It just adds more hyper parameters and makes it a little complicated to choose them. We will keep it simple and use the model as it is.\n\nWe calculate @eq-b:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#Roughness penalty parameter\nlambda <- 0.1\nI <- diag(rep(1, ncol(X)))\n\n# Estimate B\nvec_B <- solve(J_BB %x% (t(X) %*% X) + R %x% (lambda*I)) %*% as.vector(t(X) %*% C %*% J_YB)\nB <- matrix(vec_B, nrow = ncol(X), ncol = basis_beta$nbasis, byrow = TRUE)\nrownames(B) <- colnames(X)\ncolnames(B) <- basis_beta$names\n```\n:::\n\n\nThe matrix $\\hat{\\mathbf{B}}$ is:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n                bspl4.1      bspl4.2     bspl4.3      bspl4.4       bspl4.5\n(Intercept) 39.83567764 -8.156114645  1.55353932  4.170696041  6.806210e-01\nRegion1      0.02102924 -0.004393642 39.11998775 -7.952908790  1.208323e+00\nRegion2      0.80411165  0.403706075  0.03405971 -0.004664724  3.840090e+01\nRegion3      1.18986221 -0.488994883  0.89743472  0.575838801 -8.265067e-02\nRegion4      0.92640907  0.452066359  1.32817229  0.908084130  2.973605e-01\nRegion5     27.64718777 -5.904961425  0.62669004  0.377129339  2.928214e+00\nRegion6      0.05377537 -0.003575104 35.37244817 -2.954141379  6.068074e-01\nLatitude    -1.11246941  0.685761017 -0.09509752 -0.004857989  3.900007e+01\nLongitude    0.08130553  1.207129312 -0.57449217  0.524265553  8.413613e-05\nAltitude     1.39528481  4.038052460  0.40363516  1.071534774  2.874773e-01\n                 bspl4.6     bspl4.7      bspl4.8\n(Intercept)  1.322531623  0.31661863  0.464146680\nRegion1      4.122873505  0.62096127  0.476491056\nRegion2     -6.335644991  0.42058240  3.045438089\nRegion3     -0.004339815 28.15208724 -3.844667630\nRegion4      0.552791720 -0.05430695 -0.004169696\nRegion5      0.515731395  0.38074814  0.504537347\nRegion6      1.636776720 -0.53057597  2.072168553\nLatitude    -5.965707043  1.17529298  3.798277827\nLongitude   -0.004434386 40.79202308 -7.494007511\nAltitude     0.477407440  0.02736054 -0.004495832\n```\n\n\n:::\n:::\n\n\nOkay, this is a first result, and it looks nice, but we have chosen $K_Y = 7$, $K_\\beta = 8$ and $\\lambda = 0.1$. Some work must be done in choosing the best possible values for these guys. Since it is a little intensive (but doable), we will skip it for now and see if we can tune them up slighly if our results look strange.\n\n#### Fitted values and residuals\n\nWe will reconstruct our temperature curves for the 78 stations, as fitted values of our regression model.\n\n$$\n\\hat{\\mathbf{Y}}\n$$\n\nAttention! This curves are values are all *expected* temperature curves given fixed values for the covariates, so they must not be seen the same way as the original curves in the first plot. How do we compute residuals and residual 'sum of squares'?\n\n#### A comment on degrees of freedom\n\n### Third step - Some inferences\n\n### Built-in functions in `fda`\n",
    "supporting": [
      "SoFR_reverted_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}