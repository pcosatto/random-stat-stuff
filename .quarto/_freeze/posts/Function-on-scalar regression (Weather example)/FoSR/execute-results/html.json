{
  "hash": "511ae722cc2a5bbc5b39b36adeef9067",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Function-on-scalar regression\"\ndescription: \"Weather example\"\nauthor: \"Pedro Cosatto\"\ndate: \"July 7, 2025\"\ncategories:\n  - functional data analysis\n  - regression\nimage: \"preview.png\"\nformat:\n  html:\n    self-contained: true\n    html-math-method: mathjax\n    include-before-header:\n      - file: mathjax-config.html\nbibliography: references.bib      \n---\n\n\n\n### Introduction\n\nWe work with weather data from 71 weather stations in Argentina, from a [public data source](https://www.datos.gob.ar/), in which several variables were measured during a year. The dataset consists of monthly mean temperatures recorded over a full year, together with some basic geographical descriptors of each station.\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](FoSR_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=384}\n:::\n:::\n\n\nIn this plot we can visualize the curves, denoted by $Y_{i}(t)$. Along with this, we also have some geographical features describing each individual weather station -such as as latitude, longitude, altitude above sea level, and region.\n\nOur goal is to describe and predict *the complete temperature curve* of some given station, as a single random object, by using the information of the features. That is, what is a response *variable* in traditional regression setting will be response *function*. We could also use such a model to study with some detail **how the different explanatory variables affect the shape of the temperature curves**, by studying their derivatives and other characteristics of the curves. This models, in which the response is a random function and covariates are still treated as numeric or categorical variables, are known as **function-on-scalar (FoS)** regressions, and are within the larger field of functional data analysis.\n\nThis work is based almost entirely on Chapters 5 and 13 of Ramsay-Silverman textbook [@ramsay2005]. Our goal here is try to reproduce and put into practice some results that appear in those chapters.\n\nA typical functional data analysis first involves some type of *smoothing* of the data, since they typically appear as discretized observations, in different points of time or space, of a curve. Through this process, we can characterize each set of observations for a single experimental unit as a single smooth curve.\n\n### First step - Describing the response as a smooth function\n\nWe denote $\\mathbf{y}(t)$ as a vector observation of all our weather stations at time $t \\in \\mathcal{T}$. We can think of $\\mathcal{T}$ as the set of real numbers in $[0, 12]$, although our observations are made in discrete points. In the plot above we see the observation $\\mathbf{y}(4)$. As mentioned, the smoothing process consists of creating an observation $\\hat{\\mathbf{y}}(t)$ for *any* $t \\in \\mathcal{T}$. We can set up an additive measurement-error model as\n\n$$ \\mathbf{y}(t) = \\mathbf{c'} \\phi(t) + \\epsilon(t) $$\n\nthe vector $\\phi(t)$ contains a set of functions evaluated at $t$. This set of functions constitutes the *basis functions*, and we can choose as many as we want. Their expressions will be determined by the basis methodology we choose (typically Fourier expansions or B-splines).\n\nSuppose that we choose a total of $K_{Y}$ functions, and then we then combine them linearly with the real coefficients contained in the vector $\\mathbf{c} \\in \\mathbb{R}^{K_Y}$. The term $\\epsilon(t)$ is a random error vector, for each specific point in time. We use classic least squares as a method to estimate $\\mathbf{c}$, as described in detail in Chapter 4 of [@ramsay2005]. Let $\\mathbf{Y} \\in \\mathbb{R}^{N \\times n}$ be the temperature data matrix, where $N=71$ rows contain each weather station, observed in $n = 12$ points of time. Let $\\boldsymbol{\\Phi} \\in \\mathbb{R}^{K_{Y} \\times n}$ be the matrix that contains the basis functions evaluated in the points evaluated in the sample points. This is treated as a 'design matrix' in the classic regression terminology. Let's choose $K_Y = 8$ and we will work with Fourier basis expansions as a method to produce our basis functions (this is suitable for periodic data).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nK_y <- 8\nbasis_Y <- create.fourier.basis(rangeval = c(0,12), nbasis = K_y)\n\nPhi <- t(eval.basis(1:12, basis_Y)) #dimension K_y x n\n```\n:::\n\n\nLet $\\mathbf{C} \\in \\mathbb{R}^{N \\times K_Y}$ be the linear coefficients that link each term of the basis expansion to the individuals in the sample. The least squares estimator is:\n\n$$\n\\begin{equation}\n\\hat{\\mathbf{C}} = \\mathbf{Y} \\boldsymbol{\\Phi}' (\\boldsymbol{\\Phi}\\boldsymbol{\\Phi}')^{-1}\n\\end{equation} \n$$ {#eq-1}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nC_hat <- as.matrix(Y) %*% t(Phi) %*% solve(Phi %*% t(Phi))\n```\n:::\n\n\n**Since this matrix** $\\hat{\\mathbf{C}}$ **contains all what is needed to construct an observation** $\\mathbf{y}(t)$ **at *any point in time*, we will treat it as the core piece of information for our response variables.** For some $t_0 \\in \\mathcal{T}$, we can get $\\hat{\\mathbf{y}}(t)$ by applying the basis coefficients at a desired point $t_0$\n\n$$\n\\hat{\\mathbf{y}}(t_0) = \\hat{\\mathbf{C}} \\phi(t_0).\n$$\n\nNow, as mentioned before, this *hat versions* of $\\mathbf{y}(t)$ will be our input for the actual problem we are treating: The regression of the temperature curves with other available covariates. We will user the hats in $\\mathbf{y}$ and $Y$ as fitted values in the regression problem as well.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndense_t_grid <- seq(0,12, by = 0.1) #as much density as we want\ngrid_phi <- eval.basis(dense_t_grid, basis_Y)\nY_dense <- C_hat %*% t(grid_phi)\n```\n:::\n\n\n#### Is previous smoothing necessary?\n\nAs we mentioned before, having smooth observations will ensure better quality analysis, since we can work with derivatives. Nevertheless, **we can choose not to smooth**, as we will see later, if we consider that our grid of observed points in $\\mathcal{T}$ is *dense* enough for the application we have in mind.\n\n### Second step - Main course: The regression itself\n\nLet's look ad the ingredients we need to construct our main FoS regression.\n\n#### Input data for Y\n\nFirst, the input data for the response variable, or better speaking , data of our response functions! Here we could use either $\\mathbf{Y}$ (plain and simple) or $\\mathbf{Y}_{dense}$, as it was named before, as dense as we want. However, If we use the smoothed version, as said before, we will take $\\mathbf{\\hat{C}}$ as the actual input.\n\n#### Input data for our covariates (a.k.a. the *Xs*)\n\nInput data for our explanatory variables will be organized in a design matrix $\\mathbf{X} \\in \\mathbb{R}^{N \\times p}$. Here we choose `Region` (categorical), `Latitude`, `Longitude` and `Altitude` (all numeric). We could add interactions as well. Here, nothing differs from other classical problems. If we chose the categorical variable only, what we would be doing is nothing more than a functional version of one-way ANOVA.\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Covariate design matrix\nlatitude <- scale(Latitude, scale = TRUE)\naltitude <- scale(Altitude, scale = TRUE)\n\nX <- model.matrix(~ Region + latitude + altitude)\ncolnames(X) <- c('Mean', levels(data$Region)[1:5], 'Latitude', 'Altitude')\np <- ncol(X)\n```\n:::\n\n\nWe can have a look of what the $\\mathbf{X}$ matrix looks like:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n   Mean CENTRO CUYO NEA NOA PAMPEANA  Latitude    Altitude\n1     1      0    0   0   1        0 1.8199496  6.13642239\n2     1      0    0   0   1        0 1.6533641  0.02863808\n3     1      0    0   0   1        0 1.7533154  0.20775776\n4     1      0    0   0   1        0 1.4804324  1.76078445\n5     1      0    0   0   1        0 1.3836542  1.75881610\n6     1      0    0   1   0        0 1.4058656 -0.42211365\n7     1      0    0   1   0        0 1.2424531 -0.14654491\n8     1      0    0   0   1        0 1.0679350  0.20775776\n9     1      0    0   0   1        0 0.9203878 -0.28629762\n10    1      0    0   1   0        0 0.9711567 -0.57564480\n```\n\n\n:::\n:::\n\n\nThe observations of `Latitude` and `Altitude` were centered to ease plotting and interpretation. As values of `Latitude` decrease, the station is farther south.\n\n#### Setting up the linear model\n\nPutting everything into place we have\n\n$$\n\\mathbf{y}(t) = \\mathbf{X} \\boldsymbol{\\beta}(t) + \\boldsymbol{\\varepsilon}(t)\n$$ {#eq-model}\n\nwhere $\\beta(t) \\in \\mathbb{R}^{p}$ is now a vector function of the coefficients, that is the main parameter that we need to estimate here. The error terms have a variance $\\Sigma_e(t)= \\sigma^2(t) \\mathbf{I}_{N}$ that is supposed to be diagonal (uncorrelated errors) and homoscedastic. This means that all the observations (stations) have the same error variance at a given fixed $t$, and those errors are independent from other points in time (In many problems, this assumptions may seem too strong or unrealistic, since one would expect some kind of dynamic structure in the errors, and also some type of heteroskedasticity *between stations,* due, for example, to different measurement instruments used). Since our $\\beta(t)$ are required to be smooth functions, this time we need basis expansion\n\n$$\n\\beta(t) = \\mathbf{B} \\theta(t),\n$$\n\nwhere $\\theta(t)$ is a column vector that has $K_{\\beta}$ components (as many as we want), the matrix $\\mathbf{B} \\in \\mathbb{R}^{p \\times K_\\beta}$ has the linear coefficients to re-construct each one of the *p* terms in $\\beta(t)$. this case, since we are expecting $\\beta(t)$ to be periodic, we again choose Fourier.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Basis for beta functions\nK_beta <- 5\nbasis_beta <- create.fourier.basis(rangeval = c(0,12), nbasis = K_beta)\n```\n:::\n\n\n#### A way to measure roughness\n\nLet's introduce a roughness measure for $\\beta(t)$. This will be useful if we want control on the degree of smoothness of the coefficient function. This can be stated as\n\n$$\n\\int_{\\mathcal{T}} (L \\beta)' (L \\beta)\n$$\n\nwhere the matrix $L$ is a linear differential operator, that multiplied by $\\beta$ gives an approximation of the **second derivative**. Since we want the functions $\\beta$ not to be very rough, we would like this integral to be relatively small. Notice that, for the first time, we are integrating over $\\mathcal{T}$, which is the analogous of summing over the discrete sample space in classical regression.\n\n#### The loss function\n\nOur optimization problem is to reach\n\n$$\n\\begin{equation}\n\\hat{\\mathbf{B}} = \\min_{\\mathbf{B}} \\left\\{\n  \\int_\\mathcal{T} (\\hat{\\mathbf{C}} \\phi - \\mathbf{X} \\mathbf{B} \\theta)' (\\hat{\\mathbf{C}} \\phi - \\mathbf{X} \\mathbf{B} \\theta)\n  + \\lambda \\int_\\mathcal{T} (L \\mathbf{B} \\theta)' (L \\mathbf{B} \\theta)\n\\right\\}.\n\\end{equation}\n$$ {#eq-min} in plain words, to find the linear coefficients that we need to construct an estimate $\\hat{\\beta}(t)$, by minimizing a loss function that introduces a roughness penalty. To simplify notation, we have omitted the dependency on $t$ of the basis functions $\\theta$ and $\\phi$. Here is a summary of pre-specified things that will affect our results:\n\n-   The previous smoothing of the temperature curves, summarized in $\\mathbf{\\hat{C}}$.\n\n-   The basis functions $\\phi$ and $\\theta$ and their sizes $K_Y$ and $K_{\\beta}$.\n\n-   The linear differentiation operator $L$.\n\n-   The value of the roughness penalty parameter $\\lambda$.\n\nSome of this aspects can be see as hyper-parameters, and can be chosen with techniques as cross-validation.\n\n#### Some matrices\n\nWe define here some useful matrices, that will be used in the derivation of the estimator in the next section:\n\n$$\n\\mathbf{J}_{\\phi\\phi} = \\int \\phi \\phi', \\quad\n\\mathbf{J}_{\\theta\\theta} = \\int \\theta \\theta', \\quad\n\\mathbf{J}_{\\phi\\theta} = \\int \\phi \\theta', \\quad\n\\mathbf{R} = \\int (L\\theta)(L\\theta)'\n$$\n\nThese are all Gram matrices, with the inner products between the different basis terms, integrated over $\\mathcal{T}$. They play a role in the minimization process and in calculating the solution. They can easily be obtained with the `fda` library:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(fda)\n\n#Matrices matrices\nJ_YY <- inprod(basis_Y, basis_beta)\nJ_BB <- inprod(basis_beta, basis_beta)\nJ_YB <- inprod(basis_Y, basis_beta)\nR <- eval.penalty(basis_beta, int2Lfd(2))\n```\n:::\n\n\n#### The *slope* estimates\n\nBy calculating the derivative of the argument in @eq-min, with respect to $\\mathbf{B}$, setting it to 0, and operating with the traces of the Gram matrices above, we get\n\n$$\n\\begin{equation}\n\\text{vec}(\\hat{\\mathbf{B}}) =\n\\mathbf{U}^{-1}\n\\text{vec}(\\mathbf{X}' \\hat{\\mathbf{C}} \\mathbf{J}_{\\phi\\theta})\n\\end{equation}\n$$ {#eq-b}\n\nWhere\n\n$$\n\\mathbf{U} = \\mathbf{J}_{\\theta\\theta} \\otimes (\\mathbf{X}' \\mathbf{X}) + \\mathbf{R} \\otimes \\lambda I\n$$\n\nHere the notation $\\text{vec}$ expresses a vector that has all the elements of $\\hat{\\textbf{B}}$ stacked by columns, and $\\otimes$ is the Kronecker product (for any given matrices A and B, the Kronecker product $A \\otimes B$ is a matrix of matrices that result of multiplying each scalar element $a_{ij}$ with B). We could have chosen different penalization parameters $\\lambda_i$ for every component of $\\beta$. That changes the result slightly, but it's not difficult. It just adds more hyper parameters and makes it a little complicated to choose them. We will keep it simple and use the model as it is.\n\nWe calculate @eq-b:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#Roughness penalty parameter\nlambda <- 0.0000001\n\n# Estimate B\nvec_B <- solve(J_BB %x% (t(X) %*% X) + R %x% (lambda*diag(p))) %*% as.vector(t(X) %*% C_hat %*% J_YB)\nB <- matrix(vec_B, nrow = ncol(X), ncol = basis_beta$nbasis, byrow = FALSE)\nrownames(B) <- colnames(X)\ncolnames(B) <- basis_beta$names\n```\n:::\n\n\nThis is a first result, having chosen $K_Y =$ 8, $K_\\beta =$ 5 and $\\lambda =$ 10^{-7} with no criteria at all, just to try. Some work must be done in choosing their best possible values. Since it is a little intensive (but doable), we will skip it for now and see if we can tune them up slightly if our results look strange. We finally get our $\\hat{\\beta}(t)$:\n\n$$\n\\hat{\\beta}(t) = \\mathbf{\\hat{B}} \\theta(t).\n$$\n\n#### Plotting and analyzing the slopes estimates\n\nLet $\\theta(\\mathcal{T}_0)$ denote the basis functions evaluated at a customized grid of points in $\\mathcal{T}_0 \\subset \\mathcal{T}$. We will use the same `dense_t_grid` as our $\\mathcal{T}_0$ for plotting purposes, as we did in the first section. We plot\n\n$$\n\\hat{\\beta}(\\mathcal{T}_0) = \\mathbf{\\hat{B}} \\theta(\\mathcal{T}_0).\n$$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbeta <- B %*% t(eval.basis(dense_t_grid, basis_beta))\n```\n:::\n\n\nWe already know that $\\beta(t)$ is not a 'slope' strictly, it is a slope vector function, or a function of vector linear coefficients since some variables are categorical. Let's see what its components look like:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](FoSR_files/figure-html/unnamed-chunk-13-1.png){fig-align='center' width=768}\n:::\n:::\n\n\nThe top-left plot is the intercept function. The top-right plot contains all the differential effects of belonging to different regions. The two bottom plots show the coefficients for `Altitude` and `Latitude`. We can use this plots to draw some quick conclusions, for example:\n\n-   `Altitude` has a negative effect in temperature. In other words, stations located at higher elevations have cooler temperatures throughout the year, and that effect is more notorious in the warmer months of the year (October to March).\n\n-   The coefficients of `Latitude` are positive, which makes sense since latitude decreases, you go farther south and temperatures get colder. This effect is more intense in the spring and less intense in the winter.\n\nBoth coefficients for `Latitude` and `Altitude` have some degree of wiggliness that could potentially be controlled. Finally,\n\n-   We see that the differential effects of the different regions are dispersed around zero. We reproduce Figure 13.2 of [@ramsay2005], where the overall mean (calculated as a sample mean for every $t \\in \\mathcal{T}_0$ of $\\hat{Y}_{dense}$, and the mean conditional to every region -obtained by adding the corresponding $\\hat{\\beta}_j(t)$- are plotted to illustrate:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](FoSR_files/figure-html/unnamed-chunk-14-1.png){fig-align='center' width=480}\n:::\n:::\n\n\n#### Fitted values and degrees of freedom\n\nBefore we proceed with the inference questions, we compute and plot fitted values $\\hat{Y}_i(t)$ for the 71 stations. For a first glance of what the predictions look like, we can plot the high resolution version of the $Y_i(t)$, that is, $\\hat{Y}_i(\\mathcal{T}_0)$ for our customized dense grid $\\mathcal{T}_0$. These dense version of the fitted values are obtained by:\n\n$$\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\mathbf{\\hat{\\beta}}(\\mathcal{T}_0) = \\mathbf{X}\\mathbf{\\hat{B}} \\theta(\\mathcal{T}_0).\n$$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nY_fitted <- X %*% beta \n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](FoSR_files/figure-html/unnamed-chunk-16-1.png){fig-align='center' width=384}\n:::\n:::\n\n\nThis nice and smooth curves are all *expected* temperature curves given fixed values for the covariates, so they should not be seen the same way as the curves in the raw data (first plot).\n\nLet's now look at the original observations $\\mathbf{Y}$ and their respective $\\hat{\\mathbf{Y}}$, fitting the response only for the original sample points $\\{1, 2, ..., 12\\}$. We can find a a closed-form expression for the complete two-step linear mapping that we have made so far, $\\mathbf{Y} \\rightarrow \\hat{\\mathbf{Y}}$. The first step is the smoothing of $\\mathbf{Y}$, and the second step is obtaining the fitted values of the regression itself. Let\n\n$$\n\\boldsymbol{\\Theta} \\in \\mathbb{R}^{K_\\beta \\times n} \\ \\ \\ \\text{and the re-appearing} \\ \\ \\ \\ \\  \\boldsymbol{\\Phi} \\in \\mathbb{R}^{K_Y \\times n}\n$$\n\nbe the matrices that contain the evaluations of $\\theta$ and $\\phi$ in the elements in $\\{1, 2, ..., 12\\}$ (remember $n=12$ in this example). Following the derivations of Chapters 5 and 13 of [@ramsay2005], and also working with ideas taken from section 5.4 of [@hastie2009], we set up the two steps:\n\n1.  The matrix $\\hat{\\mathbf{C}}$ is first obtained from $\\mathbf{Y}$ by @eq-1, and then is plugged in @eq-b to get the coefficients $\\text{vec}(\\mathbf{\\hat{B}})$,\n2.  and the fitted values $\\mathbf{\\hat{Y}}$ (in *vectorized* version) are obtained by\n\n$$\n\\text{vec}(\\mathbf{\\hat{Y}}) = (\\boldsymbol{\\Theta}' \\otimes \\mathbf{X}) \\text{vec}(\\hat{\\mathbf{B}}).\n$$Putting all this together in a single operation we get\n\n$$\n\\begin{equation}\n\\text{vec}(\\hat{\\mathbf{Y}}) = \\mathbf{H} \\text{vec}(\\mathbf{Y})\n\\end{equation}\n$$ {#eq-proj}\n\nwhere\n\n$$\n\\mathbf{H} = (\\boldsymbol{\\Theta}' \\otimes \\mathbf{X}) \\mathbf{U}^{-1}(\\mathbf{M}' \\otimes \\mathbf{X}')\n$$\n\nand\n\n$$\n\\mathbf{M} = \\boldsymbol{\\Phi}' (\\boldsymbol{\\Phi} \\boldsymbol{\\Phi}')^{-1} \\mathbf{J}_{\\phi\\theta}.\n$$\n\nBeautifully, the two linear mappings are condensed into this **square symmetric mother matrix** $\\mathbf{H}$, also known as a *smoother* matrix, that constitutes a projection of the original observations onto a lower-rank subspace. Since a roughness penalization is involved, this projection is not orthogonal (it would be if we had $\\lambda = 0$, see Section 5.4 of [@hastie2009]).\n\nThe rank of this matrix can be thought as the *resolution* of the projections: Large rank implies large resolution, in other words, fitted curves are prone to over-fitting of the estimated means to the original observations. **The trace of** $\\mathbf{H}$ **is the so called *effective* degrees of freedom of the complete fitting procedure.**\n\nThe rank and degrees of freedom are not only controlled by the sizes of the expansion basis, $K_\\beta$ and $K_Y$, but also by the value of $\\lambda$: More penalization implies less over-fitting, hence, lower rank for the projection. An idea on how to determine $\\lambda$ numerically by specifying the effective degrees of freedom first is outlined in Section 5.4 of [@hastie2009]. We compute the matrices:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nTheta <- t(eval.basis(1:12, basis_beta)) \nM <- t(Phi) %*% solve(Phi %*% t(Phi)) %*% J_YB\nU <- J_BB %x% (t(X) %*% X) + R %x% (lambda*diag(p))\nH <- (t(Theta) %x% X) %*% solve(U) %*% (t(M) %x% t(X))\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#Effective degrees of freedom\ndf <- sum(diag(H))\n```\n:::\n\n\nSo, with our combination of hyper-parameters we have 39.9999998 *effective* degrees of freedom.\n\n#### Residuals and residual variance\n\nThe residuals will actually be residual functions $e_i(t)$, one for each station. These functions give us information about goodness of fit for the temperature curve of every station. To have a look of some examples of stations that had a poor fit, we can obtain\n\n$$\n\\texttt{SQe}_i = \\int_{\\mathcal{T}} e^2_i(t)dt\n$$\n\nWe run this calculations in the dense grid $\\mathcal{T}_0$, since we are interested in calculating an integral and also plotting:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#The residual functions (high resolution)\nresid <-t(Y_dense - Y_fitted)\n```\n:::\n\n\nLet's approximate the integral for $\\texttt{SQe}_i$ by obtaining the squared norm of the columns of `resid`, and choose the two stations with the worst fit.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nSQe <- apply(resid, 2, function(x) sum(x^2))\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](FoSR_files/figure-html/unnamed-chunk-21-1.png){fig-align='center' width=576}\n:::\n:::\n\n\nThe result is interesting since these are cities in which the wind has a notable influence in temperature. The first one has especially hot winds during the summer, and the second one has cold winds throughout the year. This suggests that the incorporation of a variable related to wind strength may improve the fit.\n\nWe finally get an estimate of the error standard deviation function $\\sigma(t)$, by calculating sample standard deviations of the residuals at fixed every fixed by t, that is, the rows of `resid`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsigma2 <- apply(resid, 1, sd)\n```\n:::\n\n\nWe could, as well, divide by the degrees of freedom calculated previously.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](FoSR_files/figure-html/unnamed-chunk-23-1.png){fig-align='center' width=384}\n:::\n:::\n\n\nWe can see poorer fits, on average, during the summer months.\n\n### Third step - Basic inference\n\nHaving obtained this estimators, and with little concern about a correct tuning of the hyper-parameters, we now sketch statistical inference procedures that use classical uni-variate random sampling theory.\n\n#### Point-wise confidence intervals for regression coefficients\n\nWe first find an expression for the covariance of $\\hat{\\beta}(t)$. Using the expressions above, and simplifying the notation to $\\hat{\\beta}_t$\n\n$$\n\\begin{aligned}\n\\hat{\\beta}_t = \\text{vec} \\ \\hat{\\beta}_t &= \\text{vec} \\ (\\hat{\\mathbf{B}}  \\theta_t) \\\\\n&=(\\theta_t' \\otimes I) \\ \\mathbf{U}^{-1} \\ \\text{vec}(\\hat{\\mathbf{B}}) \\\\\n&=(\\theta_t' \\otimes I) \\ \\mathbf{U}^{-1}\\ \\text{vec}(\\mathbf{X}' \\mathbf{Y \\mathbf{M}})  \\\\\n&= (\\theta_t' \\otimes I) \\ \\mathbf{U}^{-1}(\\mathbf{M}' \\otimes \\ \\mathbf{X}') \\ \\text{vec}(\\mathbf{Y})\n\\end{aligned}\n$$\n\nIn this expression, the time dependency is in the vector component $\\theta_t$. For a simultaneous calculation of many points, it can be replaced for a matrix $\\theta(\\mathcal{T_0})$ with columns as the basis functions evaluated in a specific set of points (if only the points $\\{1, 2, ..., 12\\}$ are desired, we would use $\\boldsymbol{\\Theta}$ as defined previously). In this case, the equality $\\hat{\\beta}_t = \\text{vec} \\ \\hat{\\beta}_t$ clearly does not hold. Let\n\n$$\n\\mathbf{V}_t = (\\theta_t' \\otimes I_p) \\ \\mathbf{U}^{-1}(\\mathbf{M}' \\otimes \\ \\mathbf{X}'),\n$$\n\na $p \\times Nn$ matrix, and recalling our error variance assumption in @eq-model, we obtain the variance-covariance matrix of our estimator as\n\n$$\n\\mathsf{Cov}(\\mathbf{\\hat{\\beta}})_t = \\mathbf{V}_t (I_n \\otimes \\sigma^2_t I_N) \\mathbf{V}'_t\n$$ {#eq-std-error-beta}\n\nwhere we plug-in $\\hat{\\sigma}^2(t)$ to obtain an estimation. As an example, we find the variance-covariance matrix for $t=7$ (July).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nt <- 7\ntheta_t <- t(eval.basis(t, basis_beta)) #column vector\n\n#The V matrix and its names\nV <- (t(theta_t) %x% diag(p)) %*% solve(U) %*% (t(M) %x% t(X))\nrownames(V) <- colnames(X)\n\nsigma2_t <- sigma2[which(dense_t_grid == t)]\nvcov_t <- V %*% (diag(n) %x% (sigma2_t * diag(N))) %*% t(V)\n```\n:::\n\n\nWe get, for the square roots of the diagonal:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\nTable: Standard errors of regression coefficients coefficients in t=7\n\n|      Mean|    CENTRO|      CUYO|       NEA|       NOA|  PAMPEANA|  Latitude|  Altitude|\n|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|\n| 0.0859598| 0.2409282| 0.1792654| 0.2400428| 0.2907503| 0.1489434| 0.1774507| 0.0967451|\n\n\n:::\n:::\n\n\nTo calculate confidence limits, we can use our Normality and independence assumptions to obtain the classical Z-pivot method. We now plot calculate and plot a grid of confidence intervals for the coefficients of `Altitude` and `Latitude`.\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](FoSR_files/figure-html/unnamed-chunk-27-1.png){fig-align='center' width=768}\n:::\n:::\n\n\nWe also include the confidence intervals for the overall temperature mean (coefficient $\\beta_1$).\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](FoSR_files/figure-html/unnamed-chunk-28-1.png){fig-align='center' width=384}\n:::\n:::\n\n\nWe can see that the estimation of the overall mean (model intercept) is much more precise. Clearly, we are ignoring the simultaneous error here. Bootstrapping or other re-sampling methods can be used if there is uncertainty about the theoretical assumptions.\n\n#### Mean temperature curve estimation for a new location\n\nSay we were to estimate the mean temperature curve of a new location $\\mathbf{x}_0$:\n\n-   The weather station will be located in `NOA` region.\n\n-   The altitude will be 300 meters above sea level.\n\n-   The latitude will be -20.\n\nWe first estimate the mean $\\hat{\\mu}_0$ by applying the coefficients of the model to the `newdata` vector, over a grid of points.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnew_latitude <- scale(-20, center = attr(latitude,  \"scaled:center\"), scale = attr(latitude,  \"scaled:scale\"))\nnew_altitude <- scale(300, center = attr(altitude,  \"scaled:center\"), scale = attr(altitude,  \"scaled:scale\"))\n\nnewdata <- c(1, 0, 0, 0, 1, 0, new_latitude, new_altitude)\n\nmu_predict <- apply(B %*% t(eval.basis(seq, basis_beta)),2, function(x) sum(x*newdata))\n```\n:::\n\n\nNow we find the variance of the estimation using @eq-std-error-beta.\n\n$$\n\\begin{aligned}\n\\mathsf{Var}(\\hat{\\mu}_0)_t &= \\mathsf{Var}(\\mathbf{x}_0'\\hat{\\beta}_t) \\\\\n&= \\mathbf{x}_0' \\mathsf{Cov}(\\hat{\\beta}_t) \\mathbf{x}_0 \\\\\n&= \\mathbf{x}_0' \\mathbf{V}_t (I_n \\otimes \\sigma^2_t I_N) \\mathbf{V}'_t \\mathbf{x}_0\n\\end{aligned}\n$$\n\nAnd we can plug-in again for $\\hat{\\sigma}_t$ to obtain the standard error. In the plot we show the estimation with the grid of point-wise confidence bands, using the Normal pivot again.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](FoSR_files/figure-html/unnamed-chunk-30-1.png){fig-align='center' width=384}\n:::\n:::\n\n\nThe dashed line corresponds to the overall mean (that would be the estimation without model), so we can see that the incorporation of the covariates is somehow useful.\n\n### Further topics\n\n-   Hyper-parameter tuning, to control roughness of the estimators appropriately.\n\n-   More advanced inferential methods, as using integrals or derivatives of the fitted curves.\n",
    "supporting": [
      "FoSR_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}