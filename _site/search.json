[
  {
    "objectID": "posts/Function-on-scalar regression (Weather example)/FoSR.html",
    "href": "posts/Function-on-scalar regression (Weather example)/FoSR.html",
    "title": "Function-on-scalar regression",
    "section": "",
    "text": "Introduction\nWe work with weather data from 71 weather stations in Argentina, from a public data source, in which several variables were measured during a year. The dataset consists of monthly mean temperatures recorded over a full year, together with some basic geographical descriptors of each station.\n\n\n\n\n\n\n\n\n\nIn this plot we can visualize the curves, denoted by \\(Y_{i}(t)\\). Along with them, we also have some geographical features describing each individual weather station: Latitude coordinate, longitude coordinate, altitude above sea level, and region.\nOur goal is to describe and predict the complete temperature curve of some given station, as a single random object, by using the information of the features. That is, what is a response variable in traditional regression setting will be response function. We could also use such a model to study with some detail how the different explanatory variables affect the shape of the temperature curves, by studying their derivatives and other characteristics of the curves. This models, in which the response is a random function and covariables are still treated as numeric or categorical variables, are known as function-on-scalar (FoS) regressions, and are within the larger field of functional data analysis. This work is based almost entirely on Chapters 5 and 13 of Ramsay-Silverman textbook (Ramsay and Silverman 2005). Our goal here is try to reproduce and put into practice some results that appear in those chapters.\n\n\nFirst step - Describing the response as a smooth function\nA typical functional data analysis first involves some type of smoothing of the data, since they typically appear as discrete observations, in different points of time or space, of a curve. Through this process, we can characterize each set of observations for a single experimental unit as a single smooth curve.\nWe denote \\(\\mathbf{y}(t)\\) as a vector observation of all our weather stations at time \\(t \\in \\mathcal{T}\\), in the plot above we see the observation \\(\\mathbf{y}(4)\\). We can think of \\(\\mathcal{T}\\) as the set of real numbers in \\([0, 12]\\), even though, as we mentioned, we will have discretely many points in this interval where we have made observations. The smoothing process consists of producing an observation \\(\\hat{\\mathbf{y}}(t)\\) that can be obtained for any \\(t \\in \\mathcal{T}\\), not just the ones available in the sample.\nTo produce this observations, we will use basis expansions. Shortly, we pick a small set of pre-specified functions, we evaluate them in t, and we combine them linearly. By doing this, we can produce nonlinear functions of t, as complex as we want. For more details about basis expansion methodology, see Chapter 3 of (Ramsay and Silverman 2005).\nWe set up an additive measurement-error model for these observations as\n\\[ \\mathbf{y}(t) = \\mathbf{c'} \\phi(t) + \\epsilon(t), \\]\nwhere the vector \\(\\phi(t)\\) contains the set of basis functions evaluated at \\(t\\). The expressions of those functions will be determined by the basis expansion methodology we choose. The most common methodologies are B-Splines and Fourier series expansions. The latter is more appropriate to describe periodic functions, and this is the case with our temperature curves, so we will be using them. Suppose that we choose a total of \\(K_{Y}\\) functions of our Fourier expansion, and then we then combine them linearly with the real coefficients contained in the vector \\(\\mathbf{c} \\in \\mathbb{R}^{K_Y}\\). The term \\(\\epsilon(t)\\) is a random error vector, for each specific point in time.\nWe use classic least squares as a method to estimate \\(\\mathbf{c}\\), as described in detail in Chapter 4 of (Ramsay and Silverman 2005). Let \\(\\mathbf{Y} \\in \\mathbb{R}^{N \\times n}\\) be the temperature data matrix, where \\(N=71\\) rows contain each weather station, observed in \\(n = 12\\) points of time. Let \\(\\boldsymbol{\\Phi} \\in \\mathbb{R}^{K_{Y} \\times n}\\) be the matrix that contains the basis functions evaluated in the points evaluated in the sample points. This is treated as a ‘design matrix’ in the classic regression terminology. Let’s choose \\(K_Y = 10\\).\n\nK_y &lt;- 10\nbasis_Y &lt;- create.fourier.basis(rangeval = c(0,12), nbasis = K_y)\n\nPhi &lt;- t(eval.basis(1:12, basis_Y)) #dimension K_y x n\n\nLet \\(\\mathbf{C} \\in \\mathbb{R}^{N \\times K_Y}\\) be the linear coefficients that link each term of the basis expansion to the individuals in the sample. The least squares estimator is:\n\\[\n\\begin{equation}\n\\hat{\\mathbf{C}} = \\mathbf{Y} \\boldsymbol{\\Phi}' (\\boldsymbol{\\Phi}\\boldsymbol{\\Phi}')^{-1}\n\\end{equation}\n\\tag{1}\\]\n\nC_hat &lt;- as.matrix(Y) %*% t(Phi) %*% solve(Phi %*% t(Phi))\n\nSince this matrix \\(\\hat{\\mathbf{C}}\\) contains all what is needed to construct an observation \\(\\mathbf{y}(t)\\) at any point in time, we will treat it as the core piece of information for our response variables. For some \\(t_0 \\in \\mathcal{T}\\), we can get \\(\\hat{\\mathbf{y}}(t)\\) by applying the basis coefficients at a desired point \\(t_0\\)\n\\[\n\\hat{\\mathbf{y}}(t_0) = \\hat{\\mathbf{C}} \\phi(t_0).\n\\]\nNow, as mentioned before, this hat versions of \\(\\mathbf{y}(t)\\) will be our input for the actual problem we are treating: The regression of the temperature curves with other available features. We will user the hats in \\(\\mathbf{y}\\) and \\(Y\\) as fitted values in the regression problem as well.\n\ndense_t_grid &lt;- seq(0,12, by = 0.1) #as much density as we want\ngrid_phi &lt;- eval.basis(dense_t_grid, basis_Y)\nY_dense &lt;- C_hat %*% t(grid_phi)\n\nSmoothing is not always necessary, especially if the functional observations \\(Y_i(t)\\) are made in a sufficiently dense grid of points. Also, it depends on the application: Some problems require a relatively dense representation of the curves, both for visualization and estimation purposes.\n\n\nSecond step - Main course: The regression itself\nLet’s look ad the ingredients we need to construct our main functional regression.\n\nInput data for Y\nFirst, the input data of our response functions. Here we could use either \\(\\mathbf{Y}\\) (plain and simple, in the original grid of points) or \\(\\mathbf{Y}_{dense}\\), as it was named before, observed in a dense grid of points, customized by the user. If we choose to use the smoothed version it is enough to work with, \\(\\mathbf{\\hat{C}}\\) as the actual input\n\n\nInput data for our co-variables (a.k.a. the Xs)\nInput data for our explanatory variables will be organized in a design matrix \\(\\mathbf{X} \\in \\mathbb{R}^{N \\times p}\\). Here we choose Region (categorical), Latitude, Longitude and Altitude (all numeric). We could add interactions as well. Here, nothing differs from other classical regression or experimental design problems. If we chose the categorical variable only, what we would be doing is nothing more than a functional version of one-way ANOVA.\n\n# Covariate design matrix\nlatitude &lt;- scale(Latitude, scale = TRUE)\naltitude &lt;- scale(Altitude, scale = TRUE)\n\nX &lt;- model.matrix(~ Region + latitude + altitude)\ncolnames(X) &lt;- c('Mean', levels(data$Region)[1:5], 'Latitude', 'Altitude')\np &lt;- ncol(X)\n\nWe can have a look of what the \\(\\mathbf{X}\\) matrix looks like:\n\n\n   Mean CENTRO CUYO NEA NOA PAMPEANA  Latitude    Altitude\n1     1      0    0   0   1        0 1.8199496  6.13642239\n2     1      0    0   0   1        0 1.6533641  0.02863808\n3     1      0    0   0   1        0 1.7533154  0.20775776\n4     1      0    0   0   1        0 1.4804324  1.76078445\n5     1      0    0   0   1        0 1.3836542  1.75881610\n6     1      0    0   1   0        0 1.4058656 -0.42211365\n7     1      0    0   1   0        0 1.2424531 -0.14654491\n8     1      0    0   0   1        0 1.0679350  0.20775776\n9     1      0    0   0   1        0 0.9203878 -0.28629762\n10    1      0    0   1   0        0 0.9711567 -0.57564480\n\n\nThe observations of Latitude and Altitude were centered to ease plotting and interpretation. As values of Latitude decrease, the station is farther south.\n\n\nSetting up the linear model\nPutting everything into place we have\n\\[\n\\mathbf{y}(t) = \\mathbf{X} \\boldsymbol{\\beta}(t) + \\boldsymbol{\\varepsilon}(t)\n\\tag{2}\\]\nwhere \\(\\beta(t) \\in \\mathbb{R}^{p}\\) is now a vector function of the coefficients, that is the main parameter that we need to estimate here. The error terms have a variance \\(\\Sigma_e(t)= \\sigma^2(t) \\mathbf{I}_{N}\\) that is supposed to be diagonal (uncorrelated errors) and homoscedastic. This means that all the observations (stations) have the same error variance at a given fixed \\(t\\), and those errors are independent from other points in time (In many problems, this assumptions may seem too strong or unrealistic, since one would expect some kind of dynamic structure in the errors, and also some type of heteroskedasticity between stations, due, for example, to different measurement instruments used). We also add an assumption of Normality of the errors at fixed t, although this is not strictly necessary for estimation purposes.\nSince our \\(\\beta(t)\\) are required to be smooth functions, this time we need basis expansion\n\\[\n\\beta(t) = \\mathbf{B} \\theta(t),\n\\]\nwhere \\(\\theta(t)\\) is a column vector that has \\(K_{\\beta}\\) components (as many as we want), the matrix \\(\\mathbf{B} \\in \\mathbb{R}^{p \\times K_\\beta}\\) has the linear coefficients to re-construct each one of the p terms in \\(\\beta(t)\\). this case, since we are expecting \\(\\beta(t)\\) to be periodic, we again choose Fourier.\n\n# Basis for beta functions\nK_beta &lt;- 6\nbasis_beta &lt;- create.fourier.basis(rangeval = c(0,12), nbasis = K_beta)\n\n\n\nA way to measure roughness\nLet’s introduce a roughness measure for \\(\\beta(t)\\). This will be useful if we want control on the degree of smoothness of the coefficient function. This can be stated as\n\\[\n\\int_{\\mathcal{T}} (L \\beta)' (L \\beta)\n\\]\nwhere the matrix \\(L\\) is a linear differential operator, that multiplied by \\(\\beta\\) gives an approximation of the second derivative. Since we want the functions \\(\\beta\\) not to be very rough, we would like this integral to be relatively small. Notice that, for the first time, we are integrating over \\(\\mathcal{T}\\), which is the analogous of summing over the discrete sample space in classical regression.\n\n\nThe loss function\nOur optimization problem is to reach\n\\[\n\\begin{equation}\n\\hat{\\mathbf{B}} = \\min_{\\mathbf{B}} \\left\\{\n  \\int_\\mathcal{T} (\\hat{\\mathbf{C}} \\phi - \\mathbf{X} \\mathbf{B} \\theta)' (\\hat{\\mathbf{C}} \\phi - \\mathbf{X} \\mathbf{B} \\theta)\n  + \\lambda \\int_\\mathcal{T} (L \\mathbf{B} \\theta)' (L \\mathbf{B} \\theta)\n\\right\\}.\n\\end{equation}\n\\tag{3}\\] in plain words, to find the linear coefficients that we need to construct an estimate \\(\\hat{\\beta}(t)\\), by minimizing a loss function that introduces a roughness penalty. To simplify notation, we have omitted the dependency on \\(t\\) of the basis functions \\(\\theta\\) and \\(\\phi\\). Here is a summary of pre-specified things that will affect our results:\n\nThe previous smoothing of the temperature curves, summarized in \\(\\mathbf{\\hat{C}}\\).\nThe basis functions \\(\\phi\\) and \\(\\theta\\) and their sizes \\(K_Y\\) and \\(K_{\\beta}\\).\nThe linear differentiation operator \\(L\\).\nThe value of the roughness penalty parameter \\(\\lambda\\).\n\nSome of this aspects can be see as hyper-parameters, and can be chosen with techniques as cross-validation.\n\n\nSome matrices\nWe define here some useful matrices, that will be used in the derivation of the estimator in the next section:\n\\[\n\\mathbf{J}_{\\phi\\phi} = \\int \\phi \\phi', \\quad\n\\mathbf{J}_{\\theta\\theta} = \\int \\theta \\theta', \\quad\n\\mathbf{J}_{\\phi\\theta} = \\int \\phi \\theta', \\quad\n\\mathbf{R} = \\int (L\\theta)(L\\theta)'\n\\]\nThese are all Gram matrices, with the inner products between the different basis terms, integrated over \\(\\mathcal{T}\\). They play a role in the minimization process and in calculating the solution. They can easily be obtained with the fda library:\n\nlibrary(fda)\n\n#Matrices matrices\nJ_YY &lt;- inprod(basis_Y, basis_beta)\nJ_BB &lt;- inprod(basis_beta, basis_beta)\nJ_YB &lt;- inprod(basis_Y, basis_beta)\nR &lt;- eval.penalty(basis_beta, int2Lfd(2))\n\n\n\nThe regression coefficient estimates (a.k.a the \\(\\hat{\\beta}\\)).\nBy calculating the derivative of the argument in Equation 3, with respect to \\(\\mathbf{B}\\), setting it to 0, and operating with the traces of the Gram matrices above, we get\n\\[\n\\begin{equation}\n\\text{vec}(\\hat{\\mathbf{B}}) =\n\\mathbf{U}^{-1}\n\\text{vec}(\\mathbf{X}' \\hat{\\mathbf{C}} \\mathbf{J}_{\\phi\\theta}),\n\\end{equation}\n\\tag{4}\\]\nwhere\n\\[\n\\mathbf{U} = \\mathbf{J}_{\\theta\\theta} \\otimes (\\mathbf{X}' \\mathbf{X}) + \\mathbf{R} \\otimes \\lambda I.\n\\]\nHere the notation \\(\\text{vec}\\) expresses a vector that has all the elements of \\(\\hat{\\textbf{B}}\\) stacked by columns, and \\(\\otimes\\) is the Kronecker product (for any given matrices A and B, the Kronecker product \\(A \\otimes B\\) is a matrix of matrices that result of multiplying each scalar element \\(a_{ij}\\) with B).\nWe could have chosen different penalization parameters \\(\\lambda_i\\) for every component of \\(\\beta\\). That changes the result slightly, but it’s not difficult. It just adds more hyper parameters and makes it a little complicated to choose them. We will keep it simple and use the model as it is. We calculate Equation 4:\n\n#Roughness penalty parameter\nlambda &lt;- 0.001\n\n# Estimate B\nvec_B &lt;- solve(J_BB %x% (t(X) %*% X) + R %x% (lambda*diag(p))) %*% as.vector(t(X) %*% C_hat %*% J_YB)\nB &lt;- matrix(vec_B, nrow = ncol(X), ncol = basis_beta$nbasis, byrow = FALSE)\nrownames(B) &lt;- colnames(X)\ncolnames(B) &lt;- basis_beta$names\n\nThis is a first result, having chosen \\(K_Y =\\) 10, \\(K_\\beta =\\) 6 and \\(\\lambda =\\) 0.001 with no criteria at all, just to try. Some work must be done in choosing their best possible values. Since it is a little intensive (but doable), we will skip it for now and see if we can tune them up slightly if our results look strange. We finally get our \\(\\hat{\\beta}(t)\\):\n\\[\n\\hat{\\beta}(t) = \\mathbf{\\hat{B}} \\theta(t).\n\\]\n\n\nPlotting and analyzing the slopes estimates\nLet \\(\\theta(\\mathcal{T}_0)\\) denote the basis functions evaluated at a customized grid of points in \\(\\mathcal{T}_0 \\subset \\mathcal{T}\\). We can think this as a matrix containing, in columns, the vectors \\(\\theta(t)\\) for every \\(t \\in \\mathcal{T}_0\\). We will use dense_t_grid as our \\(\\mathcal{T}_0\\) for plotting purposes, as we did in the first section. We plot\n\\[\n\\hat{\\beta}(\\mathcal{T}_0) = \\mathbf{\\hat{B}} \\theta(\\mathcal{T}_0).\n\\]\n\nbeta &lt;- B %*% t(eval.basis(dense_t_grid, basis_beta))\n\nWe already know that \\(\\beta(t)\\) is not a ‘slope’ strictly, it is a slope vector function, or a function of vector linear coefficients since some variables are categorical. Let’s see what its components look like:\n\n\n\n\n\n\n\n\n\nThe top-left plot is the intercept function. The top-right plot contains all the differential effects of belonging to different regions. The two bottom plots show the coefficients for Altitude and Latitude. We can use this plots to draw some quick conclusions, for example:\n\nAltitude has a negative effect in temperature. In other words, stations located at higher elevations have cooler temperatures throughout the year, and that effect is more notorious in the warmer months of the year (October to March).\nThe coefficients of Latitude are positive, which makes sense since latitude decreases, you go farther south and temperatures get colder. This effect is more intense in the spring and less intense in the winter.\n\nBoth coefficients for Latitude and Altitude have some degree of wiggliness that could potentially be controlled. Finally,\n\nWe see that the differential effects of the different regions are dispersed around zero. We reproduce Figure 13.2 of (Ramsay and Silverman 2005) where the overall mean, that is, our intercept \\(\\beta_1(t)\\), is plotted to illustrate, with all the region differential effects added or substracted:\n\n\n\n\n\n\n\n\n\n\n\n\nFitted values and degrees of freedom\nBefore we proceed with the inference questions, we compute and plot fitted values \\(\\hat{Y}_i(t)\\) for the 71 stations. For a first look of those predictors, we can plot the high resolution version of the \\(Y_i(t)\\), that is, \\(\\hat{Y}_i(\\mathcal{T}_0)\\) for our customized dense grid \\(\\mathcal{T}_0\\). These dense version of the fitted values are obtained by:\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\mathbf{\\hat{\\beta}}(\\mathcal{T}_0) = \\mathbf{X}\\mathbf{\\hat{B}} \\theta(\\mathcal{T}_0).\n\\]\n\nY_fitted &lt;- X %*% beta \n\n\n\n\n\n\n\n\n\n\nThis nice and smooth curves are all expected temperature curves given fixed values for the covariables, so they should not be seen the same way as the curves in the raw data (first plot).\n\n\nFitted values via the smoother matrix\nLet’s now look at the original observations \\(\\mathbf{Y}\\) and their respective \\(\\hat{\\mathbf{Y}}\\), fitting the response only for the original sample points \\(\\{1, 2, ..., 12\\}\\). We can find a a closed-form expression for the complete two-step linear mapping that we have made so far, \\(\\mathbf{Y} \\rightarrow \\hat{\\mathbf{Y}}\\). The first step is the smoothing of \\(\\mathbf{Y}\\), and the second step is obtaining the fitted values of the regression itself. Let\n\\[\n\\boldsymbol{\\Theta} \\in \\mathbb{R}^{K_\\beta \\times n} \\ \\ \\ \\text{and the re-appearing} \\ \\ \\ \\ \\  \\boldsymbol{\\Phi} \\in \\mathbb{R}^{K_Y \\times n}\n\\]\nbe the matrices that contain the evaluations of \\(\\theta\\) and \\(\\phi\\) in the elements in \\(\\{1, 2, ..., 12\\}\\) (remember \\(n=12\\) in this example). Following the derivations of Chapters 5 and 13 of (Ramsay and Silverman 2005), and also working with ideas taken from section 5.4 of (Hastie, Tibshirani, and Friedman 2009), we set up the two steps:\n\nThe matrix \\(\\hat{\\mathbf{C}}\\) is first obtained from \\(\\mathbf{Y}\\) by Equation 1, and then is plugged in Equation 4 to get the coefficients \\(\\text{vec}(\\mathbf{\\hat{B}})\\),\nand the fitted values \\(\\mathbf{\\hat{Y}}\\) (in vectorized version) are obtained by\n\n\\[\n\\text{vec}(\\mathbf{\\hat{Y}}) = (\\boldsymbol{\\Theta}' \\otimes \\mathbf{X}) \\text{vec}(\\hat{\\mathbf{B}}).\n\\]Putting all this together in a single operation we get\n\\[\n\\begin{equation}\n\\text{vec}(\\hat{\\mathbf{Y}}) = \\mathbf{H} \\text{vec}(\\mathbf{Y}),\n\\end{equation}\n\\tag{5}\\]\nwhere\n\\[\n\\mathbf{H} = (\\boldsymbol{\\Theta}' \\otimes \\mathbf{X}) \\mathbf{U}^{-1}(\\mathbf{M}' \\otimes \\mathbf{X}')\n\\]\nand\n\\[\n\\mathbf{M} = \\boldsymbol{\\Phi}' (\\boldsymbol{\\Phi} \\boldsymbol{\\Phi}')^{-1} \\mathbf{J}_{\\phi\\theta}.\n\\]\nBeautifully, the two linear mappings are condensed into this square symmetric mother matrix \\(\\mathbf{H}\\), also known as a smoother matrix, that constitutes a projection of the original observations onto a lower-rank subspace. Since a roughness penalization is involved, this projection is not orthogonal (it would be if we had \\(\\lambda = 0\\), see Section 5.4 of (Hastie, Tibshirani, and Friedman 2009)).\nThe rank of this matrix can be thought as the resolution of the projections: Large rank implies high resolution. In other words, curves can over-fit the original observations easier. The trace of \\(\\mathbf{H}\\) is the so called effective degrees of freedom of the complete fitting procedure.\nThe rank and degrees of freedom are not only controlled by the sizes of the expansion basis, \\(K_\\beta\\) and \\(K_Y\\), but also by the value of \\(\\lambda\\): More penalization implies less over-fitting, hence, lower rank for the projection. An idea on how to determine \\(\\lambda\\) numerically by specifying the effective degrees of freedom first is outlined in Section 5.4 of (Hastie, Tibshirani, and Friedman 2009). We compute the matrices:\n\nTheta &lt;- t(eval.basis(1:12, basis_beta)) \nM &lt;- t(Phi) %*% solve(Phi %*% t(Phi)) %*% J_YB\nU &lt;- J_BB %x% (t(X) %*% X) + R %x% (lambda*diag(p))\nH &lt;- (t(Theta) %x% X) %*% solve(U) %*% (t(M) %x% t(X))\n\n\n#Effective degrees of freedom\ndf &lt;- sum(diag(H))\n\nSo, with our combination of hyper-parameters we have 55.987914 effective degrees of freedom.\n\n\nResiduals and residual variance\nThe residuals will actually be residual functions \\(e_i(t)\\), one for each station. These functions give us information about goodness of fit for the temperature curve of every station. To have a look of some examples of stations that had a poor fit, we can obtain\n\\[\n\\texttt{SQe}_i = \\int_{\\mathcal{T}} e^2_i(t)dt\n\\]\nWe get this residuals in the dense grid \\(\\mathcal{T}_0\\), since we are interested in calculating an integral and also plotting:\n\n#The residual functions (high resolution)\nresid &lt;-t(Y_dense - Y_fitted)\n\nLet’s approximate the integral for \\(\\texttt{SQe}_i\\) by obtaining the squared norm of the columns of resid, and choose the two stations with the worst fit.\n\nSQe &lt;- apply(resid, 2, function(x) sum(x^2))\n\n\n\n\n\n\n\n\n\n\nThe result is interesting since these are cities in which the wind has a notable influence in temperature. The first one has especially hot winds during the summer, and the second one has cold winds throughout the year. This suggests that the incorporation of a variable related to wind strength may improve the fit.\nWe finally get an estimate of the error standard deviation function \\(\\sigma(t)\\), by calculating sample standard deviations of the residuals at fixed every fixed by t, that is, the rows of resid\n\nsigma2 &lt;- apply(resid, 1, sd)\n\nWe could, as well, divide by the degrees of freedom calculated previously.\n\n\n\n\n\n\n\n\n\nWe can see poorer fits, on average, during the summer months.\n\n\n\nThird step - Basic inference\nHaving obtained this estimators, and with little concern about a correct tuning of the hyper-parameters, we now sketch statistical inference procedures that use classical uni-variate random sampling theory.\n\nPoint-wise confidence intervals for regression coefficients\nWe first find an expression for the covariance of \\(\\hat{\\beta}(t)\\). Using the expressions above, and simplifying the notation to \\(\\hat{\\beta}_t\\)\n\\[\n\\begin{aligned}\n\\hat{\\beta}_t = \\text{vec} \\ \\hat{\\beta}_t &= \\text{vec} \\ (\\hat{\\mathbf{B}}  \\theta_t) \\\\\n&=(\\theta_t' \\otimes I) \\ \\mathbf{U}^{-1} \\ \\text{vec}(\\hat{\\mathbf{B}}) \\\\\n&=(\\theta_t' \\otimes I) \\ \\mathbf{U}^{-1}\\ \\text{vec}(\\mathbf{X}' \\mathbf{Y \\mathbf{M}})  \\\\\n&= (\\theta_t' \\otimes I) \\ \\mathbf{U}^{-1}(\\mathbf{M}' \\otimes \\ \\mathbf{X}') \\ \\text{vec}(\\mathbf{Y})\n\\end{aligned}\n\\]\nIn this expression, the time dependency is in the vector component \\(\\theta_t\\). For a simultaneous calculation of many points, it can be replaced for a matrix \\(\\theta(\\mathcal{T_0})\\) (if only the points \\(\\{1, 2, ..., 12\\}\\) are desired, we would use \\(\\boldsymbol{\\Theta}\\) as defined previously). In this case, the equality \\(\\hat{\\beta}_t = \\text{vec} \\ \\hat{\\beta}_t\\) clearly does not hold. Let\n\\[\n\\mathbf{V}_t = (\\theta_t' \\otimes I_p) \\ \\mathbf{U}^{-1}(\\mathbf{M}' \\otimes \\ \\mathbf{X}'),\n\\]\na \\(p \\times Nn\\) matrix, and recalling our error variance assumption in Equation 2, we obtain the variance-covariance matrix of our estimator as\n\\[\n\\mathsf{Cov}(\\mathbf{\\hat{\\beta}})_t = \\mathbf{V}_t (I_n \\otimes \\sigma^2_t I_N) \\mathbf{V}'_t\n\\tag{6}\\]\nwhere we plug-in \\(\\hat{\\sigma}^2(t)\\) to obtain an estimation. As an example, we find the variance-covariance matrix for \\(t=7\\) (July).\n\nt &lt;- 7\ntheta_t &lt;- t(eval.basis(t, basis_beta)) #column vector\n\n#The V matrix and its names\nV &lt;- (t(theta_t) %x% diag(p)) %*% solve(U) %*% (t(M) %x% t(X))\nrownames(V) &lt;- colnames(X)\n\nsigma2_t &lt;- sigma2[which(dense_t_grid == t)]\nvcov_t &lt;- V %*% (diag(n) %x% (sigma2_t * diag(N))) %*% t(V)\n\nWe get, for the square roots of the diagonal:\n\n\n\nStandard errors of regression coefficients coefficients in t=7\n\n\n\n\n\n\n\n\n\n\n\n\nMean\nCENTRO\nCUYO\nNEA\nNOA\nPAMPEANA\nLatitude\nAltitude\n\n\n\n\n0.1012032\n0.2835913\n0.2110387\n0.2825286\n0.3421546\n0.1753314\n0.2088358\n0.1138996\n\n\n\n\n\nTo calculate confidence limits, we can use our Normality and independence assumptions to obtain the classical Z-pivot method. We now plot calculate and plot a grid of confidence intervals for the coefficients of Altitude and Latitude.\n\n\n\n\n\n\n\n\n\nWe also include the confidence intervals for the overall temperature mean (coefficient \\(\\beta_1\\)).\n\n\n\n\n\n\n\n\n\nWe can see that the estimation of the overall mean (model intercept) is much more precise. Clearly, we are ignoring the simultaneous error here. Bootstrapping or other re-sampling methods can be used if there is uncertainty about the theoretical assumptions.\n\n\nMean temperature curve estimation for a new location\nSay we were to estimate the mean temperature curve of a new location \\(\\mathbf{x}_0\\):\n\nThe weather station will be located in NOA region.\nThe altitude will be 300 meters above sea level.\nThe latitude will be -20.\n\nWe first estimate the mean \\(\\hat{\\mu}_0\\) by applying the coefficients of the model to the newdata vector, over a grid of points.\n\nnew_latitude &lt;- scale(-20, center = attr(latitude,  \"scaled:center\"), scale = attr(latitude,  \"scaled:scale\"))\nnew_altitude &lt;- scale(300, center = attr(altitude,  \"scaled:center\"), scale = attr(altitude,  \"scaled:scale\"))\n\nnewdata &lt;- c(1, 0, 0, 0, 1, 0, new_latitude, new_altitude)\n\nmu_predict &lt;- apply(B %*% t(eval.basis(seq, basis_beta)),2, function(x) sum(x*newdata))\n\nNow we find the variance of the estimation using Equation 6.\n\\[\n\\begin{aligned}\n\\mathsf{Var}(\\hat{\\mu}_0)_t &= \\mathsf{Var}(\\mathbf{x}_0'\\hat{\\beta}_t) \\\\\n&= \\mathbf{x}_0' \\mathsf{Cov}(\\hat{\\beta}_t) \\mathbf{x}_0 \\\\\n&= \\mathbf{x}_0' \\mathbf{V}_t (I_n \\otimes \\sigma^2_t I_N) \\mathbf{V}'_t \\mathbf{x}_0\n\\end{aligned}\n\\]\nAnd we can plug-in again for \\(\\hat{\\sigma}_t\\) to obtain the standard error. In the plot we show the estimation with the grid of point-wise confidence bands, using the Normal pivot again.\n\n\n\n\n\n\n\n\n\nThe dashed line corresponds to the overall mean (that would be the estimation without model), so we can see that the incorporation of the covariates is somehow useful.\n\n\n\nFurther topics\n\nHyper-parameter tuning, to control roughness of the estimators appropriately.\nMore advanced inferential methods, as using integrals or derivatives of the fitted curves.\n\n\n\n\n\n\nReferences\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. Springer Series in Statistics. Springer. https://doi.org/10.1007/978-0-387-84858-7.\n\n\nRamsay, James O., and Bernard W. Silverman. 2005. Functional Data Analysis. 2nd ed. Springer."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this page",
    "section": "",
    "text": "Some random stuff that I study and share. Most of the things are drafts, articles about some model or methodology I tried, or a simulation I made, inspired by classes or research projects.\nIf you find errors, or have any comments or something you would like to share with me, this is my email contact: cosat003@umn.edu"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome - Posts index",
    "section": "",
    "text": "Function-on-scalar regression\n\n\n\nfunctional data analysis\n\nregression\n\n\n\nWeather example\n\n\n\n\n\nJul 7, 2025\n\n\nPedro Cosatto\n\n\n\n\n\nNo matching items"
  }
]